# Lumen â€“ Language Learning Agent ğŸš€

Lumen is an **LLM-powered Language Learning Agent** built to automate vocabulary learning workflows.
It can generate vocabulary lists, translate words, and automatically create **Anki decks & flashcards** using **Model Context Protocol (MCP)**.

This project is designed as an end-to-end pipeline, starting from raw multilingual word lists â†’ cleaning & preprocessing â†’ difficulty classification â†’ interactive ReAct agent â†’ Anki deck automation.

---

## âœ¨ Features

- Generate random vocabulary words in supported languages  
- Generate difficulty-aware vocabulary lists (**beginner / intermediate / advanced**)  
- Translate generated vocabulary into a target language  
- Create and manage Anki decks automatically  
- Create flashcards inside Anki using MCP integration (AnkiConnect backend)  
- Built using **LangGraph ReAct Agent workflow**  
- Supports Groq + Ollama multi-model architecture  
- Includes complete NLP preprocessing pipeline for cleaning word datasets  

---

## ğŸ§  Tech Stack

### Agent / LLM Framework
- **LangGraph**
- **LangChain**
- ReAct Agent Pattern

### LLM Providers
- **Groq API** (`llama-3.3-70b-versatile`) â†’ reasoning + tool calling  
- **Ollama** (`llama3.2:3b`) â†’ translations (local model)  

### NLP + Data Processing
- spaCy
- wordfreq
- Zipfâ€™s Law based filtering
- Lemmatization pipeline
- JSON based cleaned word lists

### Anki Integration
- **AnkiConnect**
- **Model Context Protocol (MCP)**
- **clanki** MCP server

### Backend / API
- FastAPI (for sending prompts to the agent)

---

## ğŸ“‚ Project Structure

```
Lumen-Language-Learning-Agent/
â”‚
â”œâ”€â”€ agent/                      # LangGraph agent logic + tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ tools.py
â”‚
â”œâ”€â”€ clanki/                      # MCP server integration for Anki automation
â”‚
â”œâ”€â”€ config/                      # Configuration files
â”‚
â”œâ”€â”€ data/                        # Cleaned JSON word datasets (language-wise)
â”‚
â”œâ”€â”€ logs/                        # Logging output
â”‚
â”œâ”€â”€ notebooks/                   # Experiments / pipeline notebooks
â”‚
â”œâ”€â”€ pipeline/                    # NLP preprocessing pipeline modules
â”‚
â”œâ”€â”€ raw-word-list/               # Raw multilingual vocabulary word lists
â”‚
â”œâ”€â”€ src/                         # Scripts for data ingestion and processing
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â””â”€â”€ download_spacy_models.py
â”‚
â”œâ”€â”€ utils/                       # Helper utilities (logging, configs, etc.)
â”‚
â”œâ”€â”€ assistant-groq.py            # Agent execution using Groq LLM
â”œâ”€â”€ assistant-ollama.py          # Agent execution using Ollama LLM
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸŒ Supported Languages

Currently supported languages:

- **English**
- **German**
- **Spanish**

Datasets for these languages are stored inside:

```
data/<language>/word-list-cleaned.json
```

---

## ğŸ›  Custom Tools

The agent uses 3 core custom tools:

### 1ï¸âƒ£ `get_n_random_words`
Fetches **N random words** from the cleaned dataset of a given language.

**Example:**
> Get 10 words in German.

---

### 2ï¸âƒ£ `get_n_random_words_by_difficulty_level`
Fetches **N random words filtered by difficulty level**.

Difficulty levels:
- beginner
- intermediate
- advanced

**Example:**
> Get 15 beginner words in Spanish.

---

### 3ï¸âƒ£ `translate_words`
Translates a list of words from a **source language** to a **target language** using an LLM.

**Example:**
> Translate 10 Spanish words to English.

---

## ğŸ“Œ Prerequisites

Before running the agent, make sure you have:

### âœ… Python
- Python **3.10+** recommended (works on 3.12 too)

### âœ… Node.js
- Node.js **16+** required (for MCP clanki integration)

### âœ… Anki
- Install Anki Desktop
- Install **AnkiConnect** add-on  
  Add-on code:

```
2055492159
```

Restart Anki after installation.

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Sumit-Prasad01/Lumen-Language-Learning-Agent.git
cd Lumen-Language-Learning-Agent
```

### 2ï¸âƒ£ Create virtual environment
```bash
python -m venv .lumen-env
source .lumen-env/bin/activate   # Linux/Mac
.lumen-env\Scripts\activate    # Windows
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Environment Setup

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸ§ª Running the Agent

### â–¶ Run with Groq (Reasoning + Tool Calling)
```bash
python assistant-groq.py
```

### â–¶ Run with Ollama (Local Translation Model)
Make sure Ollama is installed and model is pulled:

```bash
ollama pull llama3.2:3b
```

Then run:

```bash
python assistant-ollama.py
```

---

## ğŸ§© Anki Deck Automation (MCP + clanki)

This project uses **Model Context Protocol (MCP)** with the **clanki** MCP server to interact with Anki.

Workflow:
1. Agent generates vocabulary words  
2. Agent optionally translates them  
3. Agent creates an Anki deck (`create-deck`)  
4. Agent creates flashcards (`create-card`)  

âš ï¸ Important:
- Anki must be open and running.
- AnkiConnect must be installed.
- AnkiConnect runs at:

```
http://127.0.0.1:8765
```

---

## ğŸ§  NLP Data Cleaning Pipeline

The pipeline converts raw word lists into structured cleaned JSON datasets.

Key steps:

- Inspect and debug raw vocabulary data
- Remove noise and invalid tokens
- Lemmatize words using spaCy transformer models
- Filter rare/uncommon words using Zipfâ€™s Law
- Frequency analysis using `wordfreq`
- Build a full NLP pipeline
- Convert cleaned data into JSON
- Validate results with Spanish dataset
- Compare raw vs cleaned data

---

## ğŸ§ª Example Prompts

### Random words
```
Get 10 random words in English
```

### Difficulty-based words
```
Get 20 beginner words in Spanish
```

### Translation
```
Get 15 intermediate words in German and translate them to English
```

### Create Anki Deck
```
Get 20 easy words in Spanish, translate them to English, and create a new Anki deck called Spanish::Easy
```

---

## ğŸŒ FastAPI Integration

A FastAPI app is included to send user prompts to the agent programmatically.

This can be extended for:
- frontend integration
- chatbot UI
- API-based Anki automation
- language learning assistant apps

---

## ğŸ“Œ Roadmap / Future Improvements

- Add more supported languages
- Add part-of-speech based filtering (noun/verb/adjective)
- Add spaced repetition scheduling support
- Add vocabulary quizzes & tests
- Add UI dashboard for learners
- Add export to CSV / PDF
- Improve tool calling reliability across providers

---

## ğŸ§¾ Credits

This project integrates:
- LangGraph + LangChain ecosystem
- Groq LLM API
- Ollama local LLM inference
- spaCy + wordfreq for NLP preprocessing
- MCP + clanki for Anki automation

---


